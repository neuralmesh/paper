i can finetune a foundation model on only my questions

then turn that into an instruct model with the conversations

maybe to improve performance we can first pass the chats i have with chatgpt through a system that converts the answers from chatgpt to more cclosely reflect my writing style. that ccan be done with just gpt

then use that to turn it into a better instruct model

then use my instruction chains and condense them to remove the hallucinations

to do that we can build a prompt that identifies question chains to clearify tha exact problem (or multiple problems if exist, certainly possible)

and then a prompt that identifies solution chains. summarize both chains and compress them to just one question and one answer

this is inspired by looking at embeddings as a latent representation of something

if we take the questions chain only as a set. the embedding that would represent it should be representing the exact question

maybe this can be greatly improved by compressing them recursively and weigh them in a way that makes sense

use orca2 now with ollama

but what if its contaminated

that is the syntetic data thing.

we can open source that.

it must be easy

and it must be simple


